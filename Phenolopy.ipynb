{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vegetation Phenology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "\n",
    "import os, sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datacube\n",
    "\n",
    "from scipy.signal import savgol_filter, wiener\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.tsa.seasonal import STL as stl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datacube.drivers.netcdf import write_dataset_to_netcdf\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from dea_datahandling import load_ard\n",
    "from dea_dask import create_local_dask_cluster\n",
    "from dea_plotting import display_map, rgb\n",
    "#import deafrica_temporal_statistics as ts\n",
    "\n",
    "sys.path.append('./scripts')\n",
    "import phenolopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the cluster. paste url into dask panel for more info.\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open up a datacube connection\n",
    "dc = datacube.Datacube(app='phenolopy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study area and data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set study area and time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set lat, lon (y, x) dictionary of testing areas for gdv project\n",
    "loc_dict = {\n",
    "    'yan_full':   (-22.750, 119.10),\n",
    "    'yan_full_1': (-22.725, 119.05),\n",
    "    'yan_full_2': (-22.775, 119.15),\n",
    "    'roy_full_1': (-22.487, 119.927),\n",
    "    'roy_full_2': (-22.487, 120.092),\n",
    "    'roy_full_3': (-22.623, 119.927),\n",
    "    'roy_full_4': (-22.623, 120.092),\n",
    "    'oph_full_1': (-23.375319, 119.859309),\n",
    "    'oph_full_2': (-23.185611, 119.859309),\n",
    "    'oph_full_3': (-23.233013, 119.859309),\n",
    "    'oph_full_4': (-23.280432, 119.859309),\n",
    "    'oph_full_5': (-23.327867, 119.859309),\n",
    "    'test':       (-31.6069288, 116.9426373)\n",
    "}\n",
    "\n",
    "# set buffer length and height (x, y)\n",
    "buf_dict = {\n",
    "    'yan_full':   (0.15, 0.075),\n",
    "    'yan_full_1': (0.09, 0.025),\n",
    "    'yan_full_2': (0.05, 0.0325),\n",
    "    'roy_full_1': (0.165209/2, 0.135079/2),\n",
    "    'roy_full_2': (0.165209/2, 0.135079/2),\n",
    "    'roy_full_3': (0.165209/2, 0.135079/2),\n",
    "    'roy_full_4': (0.165209/2, 0.135079/2),\n",
    "    'oph_full_1': (0.08, 0.047452/2),\n",
    "    'oph_full_2': (0.08, 0.047452/2),\n",
    "    'oph_full_3': (0.08, 0.047452/2),\n",
    "    'oph_full_4': (0.08, 0.047452/2),\n",
    "    'oph_full_5': (0.08, 0.047452/2),\n",
    "    'test':       (0.075, 0.065)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select location from dict\n",
    "study_area = 'test'\n",
    "\n",
    "# set buffer size in lon, lat (x, y)\n",
    "lon_buff, lat_buff = buf_dict[study_area][0], buf_dict[study_area][1]\n",
    "\n",
    "# select time range. for a specific year, set same year with month 01 to 12. multiple years will be averaged.\n",
    "time_range = ('2015', '2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a study area from existing dict\n",
    "lat, lon = loc_dict[study_area][0], loc_dict[study_area][1]\n",
    "\n",
    "# combine centroid with buffer to form study boundary\n",
    "lat_extent = (lat - lat_buff, lat + lat_buff)\n",
    "lon_extent = (lon - lon_buff, lon + lon_buff)\n",
    "\n",
    "# display onto interacrive map\n",
    "display_map(x=lon_extent, y=lat_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentinel-2a, b data for above parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set measurements (bands)\n",
    "measurements = [\n",
    "    'nbart_blue',\n",
    "    'nbart_green',\n",
    "    'nbart_red',\n",
    "    'nbart_nir_1',\n",
    "    'nbart_swir_2'\n",
    "]\n",
    "\n",
    "# create query from above and expected info\n",
    "query = {\n",
    "    'x': lon_extent,\n",
    "    'y': lat_extent,\n",
    "    'time': time_range,\n",
    "    'measurements': measurements,\n",
    "    'output_crs': 'EPSG:3577',\n",
    "    'resolution': (-10, 10),\n",
    "    'group_by': 'solar_day',\n",
    "}\n",
    "\n",
    "# load sentinel 2 data\n",
    "ds = load_ard(\n",
    "    dc=dc,\n",
    "    products=['s2a_ard_granule', 's2b_ard_granule'],\n",
    "    min_gooddata=0.90,\n",
    "    dask_chunks={'time': 1},\n",
    "    **query\n",
    ")\n",
    "\n",
    "# display dataset\n",
    "print(ds)\n",
    "\n",
    "# display a rgb data result of temporary resampled median \n",
    "#rgb(ds.resample(time='1M').median(), bands=['nbart_red', 'nbart_green', 'nbart_blue'], col='time', col_wrap=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conform DEA band names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes our dask ds and conforms (renames) bands\n",
    "ds = phenolopy.conform_dea_band_names(ds)\n",
    "\n",
    "# display dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate vegetation index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes our dask ds and calculates veg index from spectral bands\n",
    "ds = phenolopy.calc_vege_index(ds, index='mavi', drop=True)\n",
    "\n",
    "# display dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary - load MODIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = phenolopy.load_test_dataset(data_path='./data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group data by month and reduce by median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take our dask ds and group and reduce dataset in median weeks (26 for one year)\n",
    "ds = phenolopy.group(ds, group_by='month', reducer='median')\n",
    "\n",
    "# display dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers from dataset on per-pixel basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk dask to -1 to make compatible with this function\n",
    "ds = ds.chunk({'time': -1})\n",
    "\n",
    "# takes our dask ds and remove outliers from data using median method\n",
    "ds = phenolopy.remove_outliers(ds=ds, method='median', user_factor=2, z_pval=0.05)\n",
    "\n",
    "# display dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample dataset down to monthly medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes our dask ds and resamples data to bi-monthly medians\n",
    "ds = phenolopy.resample(ds, interval='2W', reducer='median')\n",
    "\n",
    "# display dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate missing (i.e. nan) values linearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk dask to -1 to make compatible with this function\n",
    "ds = ds.chunk({'time': -1})\n",
    "\n",
    "# takes our dask ds and interpolates missing values\n",
    "ds = ds.interpolate_na(dim='time', method='linear')\n",
    "\n",
    "# display dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preforming forward and back fills to fill in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth data on per-pixel basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take our dask ds and smooth using savitsky golay filter\n",
    "ds = phenolopy.smooth(ds=ds, method='savitsky', window_length=3, polyorder=1)\n",
    "\n",
    "# display dataset\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper envelope correction\n",
    "todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect number of seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Phenolometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp testing area\n",
    "%autoreload\n",
    "#da = ds['veg_index'].where(ds['time.year'] == 2017, drop=True)\n",
    "ds_phenos = phenolopy.calc_phenometrics(da=da, peak_metric='pos', base_metric='bse', method='seasonal_amplitude', factor=0.2, thresh_sides='two_sided', abs_value=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_phenos['sos_values'].plot(cmap='RdYlGn', robust=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working - sos method 1 seasonal amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working zone - calc sos eos v and d via method 1 timesat\n",
    "# Based on the seasonal amplitude, defined between the base level and the\n",
    "# maximum value for each individual season. The start occurs when the left part of the fitted\n",
    "# curve has reached a specified fraction of the amplitude, counted from the base level. The end\n",
    "# of season is defined similarly, but for the right side of the curve.\n",
    "\n",
    "# todo - this works on whole time series, 2016-2018 for eg. here only one year\n",
    "\n",
    "# map_blocks takes a da of veg_index, so set a da here\n",
    "#da_sav = ds_sav['veg_index']\n",
    "\n",
    "#x = 0\n",
    "\n",
    "# temp\n",
    "#fig = plt.figure(figsize=(10, 5))\n",
    "#ds_raw = phenolopy.load_test_dataset(data_path='./data/')\n",
    "#ds_raw = ds_raw['veg_index'].where(ds_raw['time.year'] == 2017, drop=True)\n",
    "#plt.plot(ds_raw.isel(x=x, y=0).time, ds_raw.isel(x=x, y=0), marker='.')\n",
    "\n",
    "# will then need some kind of season indexer, to do each season seperately\n",
    "#da = ds.where(ds['time.year'] == 2017, drop=True)\n",
    "#da = da['veg_index']\n",
    "\n",
    "# set user percentage from base\n",
    "#sos_eos_factor = 0.1\n",
    "\n",
    "# get max val and date\n",
    "#pos_v = da.max('time')\n",
    "#pos_d = da.isel(time=da.argmax('time'))\n",
    "\n",
    "# get min value left known pos date (todo double season issue)\n",
    "#slope_left = da.where(da['time.dayofyear'] <= pos_d['time.dayofyear'])\n",
    "#slope_right = da.where(da['time.dayofyear'] >= pos_d['time.dayofyear'])\n",
    "#plt.plot(slope_left.isel(x=x, y=0).time, slope_left.isel(x=x, y=0), marker='.')\n",
    "#plt.plot(slope_right.isel(x=x, y=0).time, slope_right.isel(x=x, y=0), marker='.')\n",
    "\n",
    "#slope_diffs = slope_left.differentiate('time')\n",
    "#pos_slope_diffs = xr.where(slope_diffs > 0, True, False)\n",
    "#pos_slope_left = slope_left.where(pos_slope_diffs)\n",
    "#plt.plot(pos_slope_left.isel(x=x, y=0).time, pos_slope_left.isel(x=x, y=0), marker='.')\n",
    "\n",
    "# get base\n",
    "#base = (slope_left.min('time') + slope_right.min('time')) / 2\n",
    "#plt.axhline(base.isel(x=x, y=0), marker='.', color='blue')\n",
    "\n",
    "# get amplitude\n",
    "#amplitude = pos_v - base\n",
    "\n",
    "# get sos_v\n",
    "#sos_v = (amplitude * sos_eos_factor) + base\n",
    "#plt.axhline(sos_v.isel(x=x, y=0), marker='.', color='pink')\n",
    "\n",
    "\n",
    "#l = (amplitude * sos_eos_factor) + slope_left.min('time')\n",
    "#r = (amplitude * sos_eos_factor) + slope_right.min('time') \n",
    "#plt.axhline(l.isel(x=x, y=0), marker='.', color='purple')\n",
    "#plt.axhline(r.isel(x=x, y=0), marker='.', color='turquoise')\n",
    "\n",
    "\n",
    "# get dists from sos\n",
    "#dists_sos_v = abs(pos_slope_left - sos_v)\n",
    "\n",
    "# if want the furthest sos val from pos (if > sos vals returned)\n",
    "#min_left = dists_sos_v.isel(time=dists_sos_v.argmin('time'))\n",
    "#temp_pos = da.isel(x=x, y=0).where(da['time'] == min_left['time'].isel(x=x, y=0))\n",
    "#plt.plot(temp_pos.time, temp_pos, marker='o', color='black', alpha=0.25)\n",
    "\n",
    "# if we want closest sos val to pos we flip instead to trick argmin\n",
    "#flip = dists_sos_v.sortby(dists_sos_v['time'], ascending=False)\n",
    "#min_right = flip.isel(time=flip.argmin('time'))\n",
    "#temp_pos_cls = da.isel(x=x, y=0).where(da['time'] == min_right['time'].isel(x=x, y=0))\n",
    "#plt.plot(temp_pos_cls.time, temp_pos_cls, marker='o', color='black', alpha=0.25)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working zone - calc sos eos v and d via method 1 timesat\n",
    "# Based on the seasonal amplitude, defined between the base level and the\n",
    "# maximum value for each individual season. The start occurs when the left part of the fitted\n",
    "# curve has reached a specified fraction of the amplitude, counted from the base level. The end\n",
    "# of season is defined similarly, but for the right side of the curve.\n",
    "\n",
    "# todo - this works on whole time series, 2016-2018 for eg. here only one year\n",
    "\n",
    "# map_blocks takes a da of veg_index, so set a da here\n",
    "#da_sav = ds_sav['veg_index']\n",
    "\n",
    "#x = 0\n",
    "\n",
    "# temp\n",
    "#fig = plt.figure(figsize=(10, 5))\n",
    "#ds_raw = phenolopy.load_test_dataset(data_path='./data/')\n",
    "#ds_raw = ds_raw['veg_index'].where(ds_raw['time.year'] == 2017, drop=True)\n",
    "#plt.plot(ds_raw.isel(x=x, y=0).time, ds_raw.isel(x=x, y=0), marker='.')\n",
    "\n",
    "# will then need some kind of season indexer, to do each season seperately\n",
    "#da = ds.where(ds['time.year'] == 2017, drop=True)\n",
    "#da = da['veg_index']\n",
    "\n",
    "# set user percentage from base\n",
    "#sos_eos_factor = 0.5\n",
    "\n",
    "# get max val and date\n",
    "#pos_v = da.max('time')\n",
    "#pos_d = da.isel(time=da.argmax('time'))\n",
    "\n",
    "# get min value left known pos date (todo double season issue)\n",
    "#slope_left = da.where(da['time.dayofyear'] <= pos_d['time.dayofyear'])\n",
    "#slope_right = da.where(da['time.dayofyear'] >= pos_d['time.dayofyear'])\n",
    "#plt.plot(slope_left.isel(x=x, y=0).time, slope_left.isel(x=x, y=0), marker='.')\n",
    "#plt.plot(slope_right.isel(x=x, y=0).time, slope_right.isel(x=x, y=0), marker='.')\n",
    "\n",
    "#slope_diffs = slope_right.differentiate('time')\n",
    "#neg_slope_diffs = xr.where(slope_diffs < 0, True, False)\n",
    "#neg_slope_right = slope_right.where(neg_slope_diffs)\n",
    "#plt.plot(neg_slope_right.isel(x=x, y=0).time, neg_slope_right.isel(x=x, y=0), marker='.')\n",
    "\n",
    "# get base\n",
    "#base = (slope_left.min('time') + slope_right.min('time')) / 2\n",
    "#plt.axhline(base.isel(x=x, y=0), marker='.', color='blue')\n",
    "\n",
    "# get amplitude\n",
    "#amplitude = pos_v - base\n",
    "\n",
    "# get sos_v\n",
    "#eos_v = (amplitude * sos_eos_factor) + base\n",
    "#plt.axhline(eos_v.isel(x=x, y=0), marker='.', color='pink')\n",
    "\n",
    "# get dists from sos\n",
    "#dists_eos_v = abs(neg_slope_right - eos_v)\n",
    "\n",
    "# if want the furthest sos val from pos (if > sos vals returned)\n",
    "#min_right = dists_eos_v.isel(time=dists_eos_v.argmin('time'))\n",
    "#temp_pos = da.isel(x=x, y=0).where(da['time'] == min_right['time'].isel(x=x, y=0))\n",
    "#plt.plot(temp_pos.time, temp_pos, marker='o', color='black', alpha=0.5)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# if want the furthest sos val from pos (if > sos vals returned)\n",
    "min_left = dists_sos_v.isel(time=dists_sos_v.argmin('time'))\n",
    "temp_pos = da.isel(x=x, y=0).where(da['time'] == min_left['time'].isel(x=x, y=0))\n",
    "plt.plot(temp_pos.time, temp_pos, marker='o', color='black', alpha=0.25)\n",
    "\n",
    "# if we want closest sos val to pos we flip instead to trick argmin\n",
    "flip = dists_sos_v.sortby(dists_sos_v['time'], ascending=False)\n",
    "min_right = flip.isel(time=flip.argmin('time'))\n",
    "temp_pos_cls = da.isel(x=x, y=0).where(da['time'] == min_right['time'].isel(x=x, y=0))\n",
    "plt.plot(temp_pos_cls.time, temp_pos_cls, marker='o', color='black', alpha=0.25)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working - plot pos, sos and eos\n",
    "#cx, cy = 0, 0\n",
    "\n",
    "#print('sos_v: {0} | sos_d: {1}'.format(sos_v.isel(x=cx, y=cy).values, sos_d.isel(x=cx, y=cy).values))\n",
    "#print('eos_v: {0} | eos_d: {1}'.format(eos_v.isel(x=cx, y=cy).values, eos_d.isel(x=cx, y=cy).values))\n",
    "\n",
    "#fig = plt.figure(figsize=(6, 5))\n",
    "#da_raw = ds_raw.where(ds_raw['time.year'] == 2016, drop=True)\n",
    "#plt.plot(da_raw['time'], da_raw['veg_index'].isel(x=cx, y=cy), label='Raw', color='black', marker='.', alpha=0.25)\n",
    "#plt.plot(da_sav['time'], da_sav.isel(x=cx, y=cy), label='SavGol', color='red', marker='.')\n",
    "\n",
    "#plt.axhline(y=pos_v.isel(x=cx, y=cy).values, linewidth=1, color='orange', linestyle='--')\n",
    "#plt.axhline(y=base.isel(x=cx, y=cy).values, linewidth=1, color='orange', linestyle='--')\n",
    "\n",
    "#plt.axvline(x=sos_d.isel(x=cx, y=cy).values, linewidth=1, color='green', linestyle='--')\n",
    "#plt.axvline(x=eos_d.isel(x=cx, y=cy).values, linewidth=1, color='green', linestyle='--')\n",
    "\n",
    "#plt.grid(color='grey', alpha=0.25, linestyle='--', linewidth=1)\n",
    "#plt.xticks(da_sav['time'].data, rotation=90)\n",
    "#plt.xlabel('Date')\n",
    "#plt.ylabel('Vegetation Index') \n",
    "\n",
    "#plt.legend()\n",
    "#plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working - sos and eos method 2 absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working zone - calc sos eos v and d via method 2 timesat\n",
    "# start/end of season occurs when the curve has reached an absolute value, defined in the\n",
    "# units of the data.\n",
    "\n",
    "# todo - this works on whole time series, 2016-2018 for eg. here only one year\n",
    "\n",
    "# map_blocks takes a da of veg_index, so set a da here\n",
    "#da_sav = ds_sav['veg_index']\n",
    "\n",
    "# will then need some kind of season indexer, to do each season seperately\n",
    "#da_sav = da_sav.where(da_sav['time.year'] == 2016, drop=True)\n",
    "\n",
    "# set user percentage from base\n",
    "#sos_absolute_val = 70\n",
    "#eos_absolute_val = 70\n",
    "\n",
    "# get max val and date\n",
    "#pos_v = da_sav.max('time')\n",
    "#pos_i = da_sav.argmax('time')    # get time index of max veg value across time per vector\n",
    "#pos_d = da_sav.isel(time=pos_i)  # select day of year for index\n",
    "\n",
    "# get min value left known pos date (todo double season issue)\n",
    "#slope_left = da_sav.where(da_sav['time'] <= pos_d['time'])\n",
    "#slope_right = da_sav.where(da_sav['time'] >= pos_d['time'])\n",
    "\n",
    "# get time index of sos value (get difference, find absolute and get index at min val)\n",
    "#sos_i = xr.ufuncs.fabs(slope_left - sos_absolute_val).argmin('time')\n",
    "#eos_i = xr.ufuncs.fabs(slope_right - eos_absolute_val).argmin('time')\n",
    "\n",
    "# select dates of sos and eos\n",
    "#sos_d = da_sav['time'].isel(time = sos_i)\n",
    "#eos_d = da_sav['time'].isel(time = eos_i)\n",
    "\n",
    "# print results\n",
    "#print('pos_v: {0}\\nbase: {1}'.format(pos_v.values, base.values))\n",
    "#print('sos_v: {0}\\nsos_i: {1}\\nsos_d: {2}\\n'.format(sos_v.values, sos_i.values, sos_d.values))\n",
    "#print('eos_v: {0}\\neos_i: {1}\\neos_d: {2}'.format(eos_v.values, eos_i.values, eos_d.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working - plot pos, sos and eos\n",
    "#cx, cy = 0, 0\n",
    "\n",
    "#print('sos_v: {0} | sos_d: {1}'.format(sos_v.isel(x=cx, y=cy).values, sos_d.isel(x=cx, y=cy).values))\n",
    "#print('eos_v: {0} | eos_d: {1}'.format(eos_v.isel(x=cx, y=cy).values, eos_d.isel(x=cx, y=cy).values))\n",
    "\n",
    "#fig = plt.figure(figsize=(6, 5))\n",
    "#da_raw = ds_raw.where(ds_raw['time.year'] == 2016, drop=True)\n",
    "#plt.plot(da_raw['time'], da_raw['veg_index'].isel(x=cx, y=cy), label='Raw', color='black', marker='.', alpha=0.25)\n",
    "#plt.plot(da_sav['time'], da_sav.isel(x=cx, y=cy), label='SavGol', color='red', marker='.')\n",
    "\n",
    "#plt.axhline(y=pos_v.isel(x=cx, y=cy).values, linewidth=1, color='orange', linestyle='--')\n",
    "#plt.axhline(y=base.isel(x=cx, y=cy).values, linewidth=1, color='orange', linestyle='--')\n",
    "\n",
    "#plt.axvline(x=sos_d.isel(x=cx, y=cy).values, linewidth=1, color='green', linestyle='--')\n",
    "#plt.axvline(x=eos_d.isel(x=cx, y=cy).values, linewidth=1, color='green', linestyle='--')\n",
    "\n",
    "#plt.grid(color='grey', alpha=0.25, linestyle='--', linewidth=1)\n",
    "#plt.xticks(da_sav['time'].data, rotation=90)\n",
    "#plt.xlabel('Date')\n",
    "#plt.ylabel('Vegetation Index') \n",
    "\n",
    "#plt.legend()\n",
    "#plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working - sos and eos method 3 relative amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working zone - calc sos eos v and d via method 3 timesat\n",
    "# based on the relative amplitude for the whole time series. This amplitude is calculated as \n",
    "# the difference between the robust mean maximum and the robust mean base level (the means of values \n",
    "# when excluding the 10 % lowest and highest values). The start/end occur when the curve has \n",
    "# reached a specified fraction of this relative amplitude. In contrast to method 1, this method will \n",
    "# generate start/end vegetation index values that are identical for all seasons of a point (pixel), \n",
    "# however, the value can vary between different points (pixels).\n",
    "\n",
    "# todo - this works on whole time series, 2016-2018 for eg. here only one year\n",
    "\n",
    "\n",
    "# map_blocks takes a da of veg_index, so set a da here\n",
    "#da_sav = ds_sav['veg_index']\n",
    "\n",
    "# will then need some kind of season indexer, to do each season seperately\n",
    "#da_sav = da_sav.where(da_sav['time.year'] == 2016, drop=True)\n",
    "\n",
    "# set user percentage from base\n",
    "#user_pct = 0.5\n",
    "\n",
    "# get max val and date\n",
    "#pos_v = da_sav.max('time')\n",
    "#pos_i = da_sav.argmax('time')    # get time index of max veg value across time per vector\n",
    "#pos_d = da_sav.isel(time=pos_i)  # select day of year for index\n",
    "\n",
    "# get min value left known pos date (todo double season issue)\n",
    "#slope_left = da_sav.where(da_sav['time'] <= pos_d['time'])\n",
    "#slope_right = da_sav.where(da_sav['time'] >= pos_d['time'])\n",
    "\n",
    "# get robust mean for max and base (10% cut off either side)\n",
    "#robust_max_mean = da_sav.quantile(dim='time', q=0.90)\n",
    "#robust_base_mean = da_sav.quantile(dim='time', q=0.10)\n",
    "\n",
    "# get amplitude\n",
    "#relative_amplitude = robust_max_mean - robust_base_mean\n",
    "\n",
    "# get sos and eos value with user percentage considered\n",
    "#sos_v = (relative_amplitude * user_pct) + robust_base_mean\n",
    "#eos_v = (relative_amplitude * user_pct) + robust_base_mean\n",
    "\n",
    "# get time index of sos value (get difference, find absolute and get index at min val)\n",
    "#sos_i = xr.ufuncs.fabs(slope_left - sos_v).argmin('time')\n",
    "#eos_i = xr.ufuncs.fabs(slope_right - eos_v).argmin('time')\n",
    "\n",
    "# select dates of sos and eos\n",
    "#sos_d = da_sav['time'].isel(time = sos_i)\n",
    "#eos_d = da_sav['time'].isel(time = eos_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working - plot pos, sos and eos\n",
    "#cx, cy = 0, 0\n",
    "\n",
    "#print('sos_v: {0} | sos_d: {1}'.format(sos_v.isel(x=cx, y=cy).values, sos_d.isel(x=cx, y=cy).values))\n",
    "#print('eos_v: {0} | eos_d: {1}'.format(eos_v.isel(x=cx, y=cy).values, eos_d.isel(x=cx, y=cy).values))\n",
    "\n",
    "#fig = plt.figure(figsize=(6, 5))\n",
    "#da_raw = ds_raw.where(ds_raw['time.year'] == 2016, drop=True)\n",
    "#plt.plot(da_raw['time'], da_raw['veg_index'].isel(x=cx, y=cy), label='Raw', color='black', marker='.', alpha=0.25)\n",
    "#plt.plot(da_sav['time'], da_sav.isel(x=cx, y=cy), label='SavGol', color='red', marker='.')\n",
    "\n",
    "#plt.axhline(y=pos_v.isel(x=cx, y=cy).values, linewidth=1, color='orange', linestyle='--')\n",
    "#plt.axhline(y=base.isel(x=cx, y=cy).values, linewidth=1, color='orange', linestyle='--')\n",
    "\n",
    "#plt.axvline(x=sos_d.isel(x=cx, y=cy).values, linewidth=1, color='green', linestyle='--')\n",
    "#plt.axvline(x=eos_d.isel(x=cx, y=cy).values, linewidth=1, color='green', linestyle='--')\n",
    "\n",
    "#plt.grid(color='grey', alpha=0.25, linestyle='--', linewidth=1)\n",
    "#plt.xticks(da_sav['time'].data, rotation=90)\n",
    "#plt.xlabel('Date')\n",
    "#plt.ylabel('Vegetation Index') \n",
    "\n",
    "#plt.legend()\n",
    "#plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working - sos and eos method 4 stl trend line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working - set up a method for calcing very basic stl for sos, eos calc\n",
    "#def func_stl(v, period, seasonal, trend, robust):\n",
    "    #return stl(v, period=period, seasonal=seasonal, trend=trend, robust=robust).fit().trend\n",
    "\n",
    "# seasonal = 7 is default, trend = None will optimise best\n",
    "#def calc_stl(da, period=48, seasonal=7, trend=None, robust=False):    \n",
    "   # f = xr.apply_ufunc(func_stl, da,\n",
    "                       #input_core_dims=[['time']], \n",
    "                       #output_core_dims=[['time']], \n",
    "                       #vectorize=True, \n",
    "                       #dask='parallelized', \n",
    "                       #output_dtypes=[np.float32],\n",
    "                       #kwargs={'period': period, 'seasonal': seasonal, \n",
    "                               #'trend': trend, 'robust': robust}) \n",
    "    #return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working zone - calc sos eos v and d via method 4 timesat\n",
    "# in the fourth method the start/end occur when the curve crosses\n",
    "# the STL trend line.\n",
    "\n",
    "# get stl trend. use timesat approach - period is num dates per year * num years total\n",
    "#period = 24 * 3 \n",
    "#ds_tnd = calc_stl(ds_sav, period=period)\n",
    "\n",
    "# map_blocks takes a da of veg_index, so set a da here\n",
    "#da_sav = ds_sav['veg_index']\n",
    "#da_tnd = ds_tnd['veg_index']\n",
    "\n",
    "# will then need some kind of season indexer, to do each season seperately\n",
    "# map_blocks takes a da of veg_index, so set a da here\n",
    "#da_sav = ds_sav['veg_index']\n",
    "\n",
    "# will then need some kind of season indexer, to do each season seperately\n",
    "#da_sav = da_sav.where(da_sav['time.year'] == 2016, drop=True)\n",
    "#da_tnd = da_tnd.where(da_tnd['time.year'] == 2016, drop=True)\n",
    "\n",
    "# get max val and date\n",
    "#pos_v = da_sav.max('time')\n",
    "#pos_i = da_sav.argmax('time')    # get time index of max veg value across time per vector\n",
    "#pos_d = da_sav.isel(time=pos_i)  # select day of year for index\n",
    "\n",
    "# get min value left known pos date (todo double season issue)\n",
    "#slope_left = da_sav.where(da_sav['time'] <= pos_d['time'])\n",
    "#slope_right = da_sav.where(da_sav['time'] >= pos_d['time'])\n",
    "\n",
    "# get time index of sos value (get difference, find absolute and get index at min val)\n",
    "#sos_i = xr.ufuncs.fabs(slope_left - da_tnd).argmin('time')\n",
    "#eos_i = xr.ufuncs.fabs(slope_right - da_tnd).argmin('time')\n",
    "\n",
    "# select dates of sos and eos\n",
    "#sos_v = da_sav.isel(time=sos_i)\n",
    "#eos_v = da_sav.isel(time=eos_i)\n",
    "\n",
    "# select dates of sos and eos\n",
    "#sos_d = da_sav['time'].isel(time=sos_i)\n",
    "#eos_d = da_sav['time'].isel(time=eos_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working - plot pos, sos and eos\n",
    "#cx, cy = 0, 0\n",
    "\n",
    "#print('sos_v: {0} | sos_d: {1}'.format(sos_v.isel(x=cx, y=cy).values, sos_d.isel(x=cx, y=cy).values))\n",
    "#print('eos_v: {0} | eos_d: {1}'.format(eos_v.isel(x=cx, y=cy).values, eos_d.isel(x=cx, y=cy).values))\n",
    "\n",
    "#fig = plt.figure(figsize=(6, 5))\n",
    "#da_raw = ds_raw.where(ds_raw['time.year'] == 2016, drop=True)\n",
    "#plt.plot(da_raw['time'], da_raw['veg_index'].isel(x=cx, y=cy), label='Raw', color='black', marker='.', alpha=0.25)\n",
    "#plt.plot(da_sav['time'], da_sav.isel(x=cx, y=cy), label='SavGol', color='red', marker='.')\n",
    "#plt.plot(da_tnd['time'], da_tnd.isel(x=cx, y=cy), label='STL Trend', color='blue', linewidth=1, linestyle='--')\n",
    "\n",
    "#plt.axhline(y=pos_v.isel(x=cx, y=cy).values, linewidth=1, color='orange', linestyle='--')\n",
    "#plt.axhline(y=base.isel(x=cx, y=cy).values, linewidth=1, color='orange', linestyle='--')\n",
    "\n",
    "#plt.axvline(x=sos_d.isel(x=cx, y=cy).values, linewidth=1, color='green', linestyle='--')\n",
    "#plt.axvline(x=eos_d.isel(x=cx, y=cy).values, linewidth=1, color='green', linestyle='--')\n",
    "\n",
    "#plt.grid(color='grey', alpha=0.25, linestyle='--', linewidth=1)\n",
    "#plt.xticks(da_sav['time'].data, rotation=90)\n",
    "#plt.xlabel('Date')\n",
    "#plt.ylabel('Vegetation Index') \n",
    "\n",
    "#plt.legend()\n",
    "#plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working - los d\n",
    "#los_d = eos_d.dt.dayofyear - sos_d.dt.dayofyear\n",
    "#los_d.plot(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working - mos date (middle of season)\n",
    "#time for the mid of the season; computed as the mean value of the times for which,\n",
    "#respectively, the left edge has increased to the 80 % level and the right edge has decreased\n",
    "#to the 80 % level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working - set up a method for calcing trapezoidal using np. better than xarray integrate, which is barebones\n",
    "# todo - make the edges straighter\n",
    "#def func_trapz(v, dx):\n",
    "    #return np.trapz(y=v, dx=dx)\n",
    "\n",
    "# seasonal = 7 is default, trend = None will optimise best\n",
    "#def calc_trapz(da, dx=1):    \n",
    "    #f = xr.apply_ufunc(func_trapz, da,\n",
    "                       #input_core_dims=[['time']], \n",
    "                       #vectorize=True, \n",
    "                       #dask='parallelized', \n",
    "                       #output_dtypes=[np.float32],\n",
    "                       #kwargs={'dx': dx}) \n",
    "    #return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#da_sav.isel(x=0, y=0).plot(marker='o')\n",
    "#da_sav.where((da_sav['time'] >= sos_d['time']) & (da_sav['time'] <= eos_d['time']), 0).isel(x=0, y=0).plot()\n",
    "#da_sav.where((da_sav['time'] >= sos_d['time']) & (da_sav['time'] <= eos_d['time']), base).isel(x=0, y=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working - lios\n",
    "# set all vals outside sos and eos to 0 (trapz likes 0s, hates nans)\n",
    "#da_lios = da_sav.where((da_sav['time'] >= sos_d['time']) & (da_sav['time'] <= eos_d['time']), 0) # here we set to 0\n",
    "#da_lios = calc_trapz(da=da_lios, dx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working - sios\n",
    "# set all vals outside sos and eos to base (trapz likes base, hates nans)\n",
    "#da_sios = da_sav.where((da_sav['time'] >= sos_d['time']) & (da_sav['time'] <= eos_d['time']), 0) # set all nans to 0\n",
    "#da_sios = xr.where(da_sios > 0, base, 0) # set everything that isnt 0 to base\n",
    "#da_sios = calc_trapz(da=v, dx=1) # we can use trapz to calc area under base, then minus from lios\n",
    "#da_sios = da_lios - da_sios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working - Prelim phenometrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# calculate value (max vege) at pos (peak of season)\n",
    "#def get_pos_v(da):\n",
    "    #pos_v = da.max('time')  # get max veg value across time per vector\n",
    "    \n",
    "    #return pos_v\n",
    "\n",
    "\n",
    "# calculate date (doy) at pos (peak of season)\n",
    "#def get_pos_d(da):\n",
    "    #i = da.argmax('time')  # get time index of max veg value across time per vector\n",
    "    #pos_d = da.isel(time=i)  # select day of year for index\n",
    "    #pos_d = pos_d['time'].dt.dayofyear  # convert to doy\n",
    "\n",
    "    #return pos_d\n",
    "\n",
    "\n",
    "# calculate value (min vege) at trh (lowest point of season)\n",
    "#def get_trh_v(da):\n",
    "    #trh_v = da.min('time')  # get min veg value across time per vector\n",
    "    \n",
    "    #return trh_v\n",
    "\n",
    "\n",
    "# calculate date (doy) at trh (trough of season)\n",
    "#def get_trh_d(da):\n",
    "    #i = da.argmin('time')  # get time index of min veg value across time per vector\n",
    "    #trh_d = da.isel(time=i)  # select day of year for index\n",
    "    #trh_d = trh_d['time'].dt.dayofyear  # convert to doy\n",
    "\n",
    "    #return trh_d\n",
    "\n",
    "\n",
    "# calculate value (vege) at sos (start of season)\n",
    "#def get_sos_v(da, pos_d, method='percentile', threshold=0.2):   \n",
    "    #greening = da.where(da['time'] <= pos_d['time'])  # select times prior to pos (greening) (changed < to <= to capture when first is highest)\n",
    "    #slope_diffs = greening.differentiate('time')  # calc second order diffs\n",
    "    #pos_diffs = slope_diffs.where(slope_diffs > 0)  # select only positive diffs (i.e. non neg)\n",
    "    \n",
    "    #pos_greening = greening.where(pos_diffs)  # select raw veg values where positive on greening slope\n",
    "    \n",
    "    #if method == 'first':\n",
    "        #slope_med = pos_greening.median('time')  # get median of raw veg on pos greening slopes\n",
    "        #dists_from_median = pos_greening - slope_med  # calc vege distances from median val\n",
    "        \n",
    "        #nan_mask = dists_from_median.isnull().all('time')  # calc mask where all vals across time is nan\n",
    "        \n",
    "        #i = dists_from_median.fillna(float(dists_from_median.max() + 1))  # fill nans by taking highest val + 1\n",
    "        #i = i.argmin(dim='time', skipna=True).where(~nan_mask).astype('int16')  # get index of lowest (furthest) dist from median\n",
    "    \n",
    "    #elif method == 'percentile':\n",
    "        #slope_pct = pos_greening.quantile(dim='time', q=threshold, interpolation ='nearest',)  # cut off lower percentile of veg values\n",
    "        #dists_from_percent = pos_greening - slope_pct  # calc vege distances from percentile val\n",
    "        #dists_from_percent_abs = xr.ufuncs.fabs(dists_from_percent)  # convert values to absolute value\n",
    "        \n",
    "        #nan_mask = dists_from_percent_abs.isnull().all('time')  # calc mask where all vals across time is nan\n",
    "        \n",
    "        #i = dists_from_percent_abs.fillna(float(dists_from_percent_abs.max() + 1))  # fill nans by taking highest val + 1\n",
    "        #i = i.argmin(dim='time', skipna=True).astype('int16')\n",
    "\n",
    "    #elif method == 'median':\n",
    "        #slope_med = pos_greening.median('time')  # get median of raw veg on pos greening slopes\n",
    "        #dists_from_median = pos_greening - slope_med  # calc vege distances from median val\n",
    "        #dists_from_median_abs = xr.ufuncs.fabs(dists_from_median)  # convert values to absolute value\n",
    "        \n",
    "        #nan_mask = dists_from_median_abs.isnull().all('time')  # calc mask where all vals across time is nan\n",
    "        \n",
    "        #i = dists_from_median_abs.fillna(float(dists_from_median_abs.max() + 1))  # fill nans by taking highest val + 1\n",
    "        #i = i.argmin(dim='time', skipna=True).where(~nan_mask).astype('int16')  # get index of lowest (furthest) dist from median\n",
    "\n",
    "    #sos_v = pos_greening.isel(time=i)\n",
    "    \n",
    "    #return sos_v\n",
    "\n",
    "\n",
    "# calculate date (doy) at sos (start of season)\n",
    "#def get_sos_d(da, sos_v):\n",
    "    #sos_d = sos_v['time'].dt.dayofyear\n",
    "    \n",
    "    #return sos_d\n",
    "\n",
    "\n",
    "# calculate value (vege) at eos (end of season)\n",
    "#def get_eos_v(da, pos_d, method='percentile', threshold=0.8):\n",
    "    #browning = da.where(da['time'] >= pos_d['time'])  # select times prior to pos (greening) (changed > to >= to capture when first is highest)\n",
    "    #slope_diffs = browning.differentiate('time')  # calc second order diffs\n",
    "    #neg_diffs = slope_diffs.where(slope_diffs < 0)  # select only negative diffs (i.e. non pos)\n",
    "    \n",
    "    #neg_browning = browning.where(neg_diffs)  # select raw veg values where negative on browning slope\n",
    "    \n",
    "    #if method == 'first':\n",
    "        #slope_med = neg_browning.median('time')  # get median of raw veg on neg browing slopes\n",
    "        #dists_from_median = neg_browning - slope_med  # calc vege distances from median\n",
    "    \n",
    "        #nan_mask = dists_from_median.isnull().all('time')  # calc mask where all vals across time is nan\n",
    "    \n",
    "        #i = dists_from_median.fillna(float(dists_from_median.max() + 1))  # fill nans by taking highest val + 1\n",
    "        #i = i.argmin(dim='time', skipna=True).where(~nan_mask).astype('int16')  # get index of lowest (furthest) dist from median\n",
    "    \n",
    "    #elif method == 'percentile':\n",
    "        #slope_pct = neg_browning.quantile(dim='time', q=threshold, interpolation ='nearest',)  # cut off lower percentile of veg values\n",
    "        #dists_from_percent = neg_browning - slope_pct  # calc vege distances from percentile val\n",
    "        #dists_from_percent_abs = xr.ufuncs.fabs(dists_from_percent)  # convert values to absolute value\n",
    "        \n",
    "        #nan_mask = dists_from_percent_abs.isnull().all('time')  # calc mask where all vals across time is nan\n",
    "        \n",
    "        #i = dists_from_percent_abs.fillna(float(dists_from_percent_abs.max() + 1))  # fill nans by taking highest val + 1\n",
    "        #i = i.argmin(dim='time', skipna=True).astype('int16')\n",
    "\n",
    "    #elif method == 'median':\n",
    "        #slope_med = neg_browning.median('time')  # get median of raw veg on neg browing slopes\n",
    "        #dists_from_median = neg_browning - slope_med  # calc vege distances from median\n",
    "        #dists_from_median_abs = xr.ufuncs.fabs(dists_from_median)  # convert values to absolute value\n",
    "    \n",
    "        #nan_mask = dists_from_median_abs.isnull().all('time')  # calc mask where all vals across time is nan\n",
    "    \n",
    "        #i = dists_from_median_abs.fillna(float(dists_from_median_abs.max() + 1))  # fill nans by taking highest val + 1\n",
    "        #i = i.argmin(dim='time', skipna=True).where(~nan_mask).astype('int16')  # get index of lowest (furthest) dist from median\n",
    "        \n",
    "    #eos_v = neg_browning.isel(time=i)\n",
    "    \n",
    "    #return eos_v\n",
    "\n",
    "\n",
    "# calculate date (doy) at eos (end of season)\n",
    "#def get_eos_d(da, eos_v, method='percentile'):\n",
    "    #eos_d = eos_v['time'].dt.dayofyear\n",
    "    \n",
    "    #return eos_d\n",
    "\n",
    "\n",
    "# calculate aos value (amplitude of season)\n",
    "#def get_aos_v(da, pos_v, trh_v):\n",
    "    #aos_v = pos_v - trh_v  # minus peak of season from lowest point of season\n",
    "    \n",
    "    #return aos_v\n",
    "\n",
    "\n",
    "# calculate los (doy) from sos (start of season) to eos (end of season)\n",
    "#def get_los_d(da, sos_d, eos_d):\n",
    "    #los_d = eos_d - sos_d  # get difference in doys between end and start of season\n",
    "    \n",
    "    #max_doy = int(da['time'].dt.dayofyear[-1])  # get max doy in da\n",
    "    #los_d = xr.where(los_d >= 0, los_d, max_doy + (eos_d.where(los < 0) - sos_d.where(los < 0)))  # correct for neg vals\n",
    "    \n",
    "    #return los_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# method = first gets first positive slope value from left (greening), first negative slope value from right (browning)\n",
    "# method = percentile gets greening or browning point above/below a user veg threshold (via percentile)\n",
    "# method = median gets greening or browning point from the median of positive vals on slope (be it pos or neg)\n",
    "\n",
    "#def calc_phenology(da):\n",
    "#return pos_v\n",
    "#pos_v = ds['veg_index'].map_blocks(get_pos_v, template=template)\n",
    "#pos_d = ds['veg_index'].map_blocks(get_pos_d, template=template)\n",
    "#trh_v = ds['veg_index'].map_blocks(get_trh_v, template=template)\n",
    "#trh_d = ds['veg_index'].map_blocks(get_trh_d, template=template)\n",
    "#sos_v = ds['veg_index'].map_blocks(get_sos_v, template=template, kwargs={'pos_d': pos_d, 'method': 'percentile', 'threshold': 0.2})\n",
    "#sos_d = ds['veg_index'].map_blocks(get_sos_d, template=template, kwargs={'sos_v': sos_v})\n",
    "#eos_v = ds['veg_index'].map_blocks(get_eos_v, template=template, kwargs={'pos_d': pos_d, 'method': 'percentile', 'threshold': 0.8})\n",
    "#eos_d = ds['veg_index'].map_blocks(get_eos_d, template=template, kwargs={'method': 'first', 'eos_v': eos_v})\n",
    "#aos_v = ds['veg_index'].map_blocks(get_aos_v, template=template, kwargs={'pos_v': pos_v, 'trh_v': trh_v})\n",
    "#los_d = ds['veg_index'].map_blocks(get_los_d, template=template, kwargs={'sos_d': sos_d, 'eos_d': eos_d})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# different types of detection, using stl residuals - remove outlier method\n",
    "#from scipy.stats import median_absolute_deviation\n",
    "\n",
    "#v = ds.isel(x=0, y=0, time=slice(0, 69))\n",
    "#v['veg_index'].data = data\n",
    "\n",
    "#v_med = remove_outliers(v, method='median', user_factor=1, num_dates_per_year=24, z_pval=0.05)\n",
    "#v_zsc = remove_outliers(v, method='zscore', user_factor=1, num_dates_per_year=24, z_pval=0.1)\n",
    "\n",
    "#stl_res = stl(v['veg_index'], period=24, seasonal=5, robust=True).fit()\n",
    "#v_rsd = stl_res.resid\n",
    "#v_wgt = stl_res.weights\n",
    "\n",
    "#o = v.copy()\n",
    "#o['veg_index'].data = v_rsd\n",
    "\n",
    "#w = v.copy()\n",
    "#w['veg_index'].data = v_wgt\n",
    "\n",
    "#m = xr.where(o > o.std('time'), True, False)\n",
    "#o = v.where(m)\n",
    "\n",
    "#m = xr.where(w < 1e-8, True, False)\n",
    "#w = v.where(m)\n",
    "\n",
    "#fig = plt.figure(figsize=(18, 7))\n",
    "#plt.plot(v['time'], v['veg_index'], color='black', marker='o')\n",
    "#plt.plot(o['time'], o['veg_index'], color='red', marker='o', linestyle='-')\n",
    "#plt.plot(w['time'], w['veg_index'], color='blue', marker='o', linestyle='-')\n",
    "#plt.axhline(y=float(o['veg_index'].std('time')))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# working method for stl outlier dection. can't quite get it to match timesat results?\n",
    "# need to speed this up - very slow for even relatively small datasets\n",
    "#def func_stl(vec, period, seasonal, jump_l, jump_s, jump_t):\n",
    "    #resid = stl(vec, period=period, seasonal=seasonal, \n",
    "                #seasonal_jump=jump_s, trend_jump=jump_t, low_pass_jump=jump_l).fit()\n",
    "    #return resid.resid\n",
    "\n",
    "#def do_stl_apply(da, multi_pct, period, seasonal):\n",
    "    \n",
    "    # calc jump size for lowpass, season and trend to speed up processing\n",
    "    #jump_l = int(multi_pct * (period + 1))\n",
    "    #jump_s = int(multi_pct * (period + 1))\n",
    "    #jump_t = int(multi_pct * 1.5 * (period + 1))\n",
    "    \n",
    "    #f = xr.apply_ufunc(func_stl, da,\n",
    "                       #input_core_dims=[['time']], \n",
    "                       #output_core_dims=[['time']], \n",
    "                       #vectorize=True, dask='parallelized', \n",
    "                       #output_dtypes=[ds['veg_index'].dtype],\n",
    "                       #kwargs={'period': period, 'seasonal': seasonal, \n",
    "                               #'jump_l': jump_l, 'jump_s': jump_s, 'jump_t': jump_t}) \n",
    "    #return f\n",
    "\n",
    "# chunk up to make use of dask parallel\n",
    "#ds = ds.chunk({'time': -1})\n",
    "\n",
    "# calculate residuals for each vector  stl\n",
    "#stl_resids = do_stl_apply(ds['veg_index'], multi_pct=0.15, period=24, seasonal=13)\n",
    "\n",
    "#s = ds['veg_index'].stack(z=('x', 'y'))\n",
    "#s = s.chunk({'time': -1})\n",
    "#s = s.groupby('z').map(func_stl)\n",
    "#out = out.unstack()\n",
    "\n",
    "#s = ds.chunk({'time': -1})\n",
    "#t = xr.full_like(ds['veg_index'], np.nan)\n",
    "#out = xr.map_blocks(func_stl, ds['veg_index'], template=t).compute()\n",
    "\n",
    "#stl_resids = stl_resids.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working double logistic - messy though\n",
    "# https://colab.research.google.com/github/1mikegrn/pyGC/blob/master/colab/Asymmetric_GC_integration.ipynb#scrollTo=upaYKFdBGEAo\n",
    "# see for asym gaussian example\n",
    "\n",
    "da = v.where(v['time.year'] == 2016, drop=True)\n",
    "\n",
    "def logi(x, a, b, c, d):\n",
    "    return a / (1 + xr.ufuncs.exp(-c * (x - d))) + b\n",
    "\n",
    "# get date at max veg index\n",
    "idx = int(da['veg_index'].argmax())\n",
    "\n",
    "# get left and right of peak of season\n",
    "da_l = da.where(da['time'] <= da['time'].isel(time=idx), drop=True)\n",
    "da_r = da.where(da['time'] >= da['time'].isel(time=idx), drop=True)\n",
    "\n",
    "# must sort right curve (da_r) descending to flip data\n",
    "da_r = da_r.sortby(da_r['time'], ascending=False)\n",
    "\n",
    "# get indexes of times (times not compat with exp)\n",
    "da_l_x_idxs = np.arange(1, len(da_l['time']) + 1, step=1)\n",
    "da_r_x_idxs = np.arange(1, len(da_r['time']) + 1, step=1)\n",
    "\n",
    "# fit curve\n",
    "popt_l, pcov_l = curve_fit(logi, da_l_x_idxs, da_l['veg_index'], method=\"trf\")\n",
    "popt_r, pcov_r = curve_fit(logi, da_r_x_idxs, da_r['veg_index'], method=\"trf\")\n",
    "\n",
    "# apply fit to original data\n",
    "da_fit_l = logi(da_l_x_idxs, *popt_l)\n",
    "da_fit_r = logi(da_r_x_idxs, *popt_r)\n",
    "\n",
    "# flip fitted vector back to original da order\n",
    "da_fit_r = np.flip(da_fit_r)\n",
    "\n",
    "# get mean of pos value, remove overlap between l and r\n",
    "pos_mean = (da_fit_l[-1] + da_fit_r[0]) / 2\n",
    "da_fit_l = np.delete(da_fit_l, -1)\n",
    "da_fit_r = np.delete(da_fit_r, 1)\n",
    "\n",
    "# concat back together with mean val inbetween\n",
    "da_logi = np.concatenate([da_fit_l, pos_mean, da_fit_r], axis=None)\n",
    "\n",
    "# smooth final curve with mild savgol\n",
    "da_logi = savgol_filter(da_logi, 3, 1)\n",
    "\n",
    "fig = plt.subplots(1, 1, figsize=(6, 4))\n",
    "plt.plot(da['time'], da['veg_index'], 'o')\n",
    "plt.plot(da['time'], da_logi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#from scipy.signal import find_peaks\n",
    "\n",
    "#x, y = 0, 1\n",
    "\n",
    "#v = da.isel(x=x, y=y)\n",
    "\n",
    "#height = float(v.quantile(dim='time', q=0.75))\n",
    "#distance = math.ceil(len(v['time']) / 4)\n",
    "\n",
    "#p = find_peaks(v, height=height, distance=distance)[0]\n",
    "\n",
    "#p_dts = v['time'].isel(time=p)\n",
    "\n",
    "#for p_dt in p_dts:\n",
    "    #plt.axvline(p_dt['time'].dt.dayofyear, color='black', linestyle='--')\n",
    "\n",
    "#count_peaks = len(num_peaks[0])\n",
    "#if count_peaks > 0:\n",
    "    #return count_peaks\n",
    "#else:\n",
    "    #return 0\n",
    "    \n",
    "#plt.plot(v['time.dayofyear'], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip to get min closest to pos\n",
    "# if we want closest sos val to pos we flip instead to trick argmin\n",
    "#flip = dists_sos_v.sortby(dists_sos_v['time'], ascending=False)\n",
    "#min_right = flip.isel(time=flip.argmin('time'))\n",
    "#temp_pos_cls = da.isel(x=x, y=0).where(da['time'] == min_right['time'].isel(x=x, y=0))\n",
    "#plt.plot(temp_pos_cls.time, temp_pos_cls, marker='o', color='black', alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roi and rod method from timesat. chad seems superior\n",
    "# cut into left and right\n",
    "\n",
    "# get value and time at 20% quantile on left then right\n",
    "# get value and time at 80% quantile on left then right\n",
    "\n",
    "# roi = (left 80% val - left 20% val) / (left 80% doy - left 20% doy)\n",
    "\n",
    "#slope_l = da.where(da['time.dayofyear'] <= da_phenos['sos_times'])\n",
    "\n",
    "#slope_l_low_values = slope_l.quantile(dim='time', q=0.2)\n",
    "#slope_l_low_abs_dists = abs(slope_l_low_values - slope_l)\n",
    "#slope_l_low_times = slope_l['time.dayofyear'].isel(time=slope_l_low_abs_dists.argmin('time'))\n",
    "\n",
    "#slope_l_high_abs_dists = abs(slope_l_high_values - slope_l)\n",
    "#slope_l_high_times = slope_l['time.dayofyear'].isel(time=slope_l_high_abs_dists.argmin('time'))\n",
    "\n",
    "#roi = (slope_l_high_values - slope_l_low_values) / (slope_l_high_times - slope_l_low_times)\n",
    "#roi.plot(cmap='RdYlGn', robust=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
